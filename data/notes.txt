Abbiamo anche sperimentato 2 metodi per la creazione del mondo, il primo,
semplicemnte lanciando un kernel sulla gpu e fare li tutte le allocazioni
e manipolazioni del caso, e il secondo, dove facciamo tutte le allocazioni
sull'host e con delle memcopy ci spostiamo gli oggetti settati, sul device.

1. Dà lavoro sequenziale alla gpu e non sfrutta il parallelismo
2. Permette un maggiore controllo sulle allocazioni e sulle copie
    citare SoA sono genericamente più veloci di AoS e quindi abbiamo provato
    questo approccio. Peccato però che il codice diventa difficile da leggere
    e si creano delle dipendenze di allocazione tra le strutture che inducono
    molto facilmente ad errori.

SHARED_MEMORY:
Abbiamo provato a usare la shared memory per aiutare l'anti-aliasing.
Di base, ciò che fa l'anti-aliasing è prendere dei sample a partire da un pixel
ed eseguire la media dei colori risultanti dai raggi. Abbiamo quindi pensato di 
ottimizzare le operazioni facendo calcolare meno sample ad ogni pixel, utilizzando
i sample dei pixel vicini per calcolare la media. 
Per fare ciò abbiamo utilizzato la shared memory, in cui ogni thread pone il proprio
risultato e poi si va a calcolare la media prendendo i risultati dei vicini.

conclusioni:
    Abbiamo notato che con questo approccio si riesce ad avere un risultato 
    simile tra l'utilizzo della shared_memory con pochi sample e il non utilizzo 
    di essa con molti sample a livelo di qualità dell'immagine.
    Con questo approccio si ha un miglioramento dei tempi di esecuzione, con 
    uno speed-up di circa 2x rispetto all'approccio senza shared_memory.

        Num.Sample: 25; Ex Time: 2.35094
        Num.Sample: 60; Ex Time: 4.95851
        allegare immagini per far vedere che sono circa uguali



WARP DIVERGENCE:
Come tutti i raytracer, anche questo soffre di warp divergence a causa della 
randomizzazione delle rifrazioni e riflessioni dei raggi. Di fatti, in un warp
alcuni thread potrebbero giungere in direzioni dove incontrano oggetti riflettenti, 
mentre altri no. In questo caso anche questi ultimi devono aspettare che i primi
finiscano di calcolare il colore totale assunto dalla riflessione. 

Abbiamo pensato di calcolare i sample in un nested kernel ed utilizzare una reduction
tra i thread per calcolare il colore totale. Per fare questo ci siamo imbattuti in
diversi problemi:
    -compilazione: utilizzare compilazione e linking separati aggiungendo librerie 
        nvcc -arch=sm_35 -rdc=true myprog.cu -lcudadevrt -o myprog.o
        nvcc -arch=sm_35 myprog.o -lcudadevrt -o myprog
    -esecuzione: essendo stata deprecata la funzione cudaDeviceSynchronize() nei 
        kernel, ci è stato impossibile avere risultati concreti.
        Abbiamo pensato anche di usare le stream, ma abbiamo visto che il lavoro
        si complicava abbastanza. Potrebbe comunque rimanere un'idea per il futuro.

nota: leggere http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf
per vedere se si può fare qualcosa di più intelligente per risolvere il problema
della warp divergence.
 